{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating decomposer with parameter data hash cache/data-1c7597d69d97d2128ba3828c410fd633cf9841c1/start.pkl\n",
      "Creating decomposer with parameter lattice hash cache/correlation-a416ec458fa5589b619bfa2849adfc7b746abab8/start.pkl\n",
      "Created dataset\n"
     ]
    }
   ],
   "source": [
    "import src.decorrelate as cluster_model\n",
    "MODEL_NAME = 'EleutherAI/pythia-70m'\n",
    "DATASET_NAME = 'NeelNanda/pile-10k'\n",
    "\n",
    "N_DIMS = 512\n",
    "SEED = 69_420\n",
    " \n",
    "DEBUG = False\n",
    " \n",
    "if DEBUG:\n",
    "    N_DATASIZE = 300\n",
    "    N_BLOCKS = 12\n",
    "    STRING_SIZE_CUTOFF = 200\n",
    "else:\n",
    "    # It gets killed aroun 1_800 idk why. Maybe we have a problem with token truncation somewhere\n",
    "    N_DATASIZE = 2_001 # TODO: was 3_000 for non-digraph\n",
    "# \n",
    "    # N_CLUSTERS_MIN = int(0.5 * N_DIMS)\n",
    "    # N_CLUSTERS_MAX = 10 * N_DIMS\n",
    "    # TODO: DEL ME\n",
    "    N_BLOCKS = 6\n",
    "    STRING_SIZE_CUTOFF = 1_000\n",
    "\n",
    "params = cluster_model.InterpParams(\n",
    "\tlattice_params=cluster_model.LatticeParams(\n",
    "\t\ttop_layer_idx = -1,\n",
    "        max_n_parents = 4\n",
    "\t),\n",
    "    # quantization='4bit',\n",
    "\tseed=SEED,\n",
    "    n_datasize=N_DATASIZE,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    model_name=MODEL_NAME,\n",
    "\tmodel_n_dims=N_DIMS,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    string_size_cutoff=STRING_SIZE_CUTOFF,\n",
    "    quantization='4bit'\n",
    ")\n",
    "\n",
    "decomp = cluster_model.Decomposer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   class GPTNeoXLayer(nn.Module):\\n    def __init__(self, config):\\n        super().__init__()\\n        self.use_parallel_residual = config.use_parallel_residual\\n        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\\n        self.post_attention_dropout = nn.Dropout(config.hidden_dropout)\\n        self.post_mlp_dropout = nn.Dropout(config.hidden_dropout)\\n        self.attention = GPTNeoXAttention(config)\\n        self.mlp = GPTNeoXMLP(config)\\n\\n    def forward(\\n        self,\\n        hidden_states: Optional[torch.FloatTensor],\\n        attention_mask: Optional[torch.FloatTensor] = None,\\n        position_ids: Optional[torch.LongTensor] = None,\\n        head_mask: Optional[torch.FloatTensor] = None,\\n        use_cache: Optional[bool] = False,\\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\\n        output_attentions: Optional[bool] = False,\\n    ):\\n        attention_layer_outputs = self.attention(\\n            self.input_layernorm(hidden_states),\\n            attention_mask=attention_mask,\\n            position_ids=position_ids,\\n            layer_past=layer_past,\\n            head_mask=head_mask,\\n            use_cache=use_cache,\\n            output_attentions=output_attentions,\\n        )\\n        attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\\n        attn_output = self.post_attention_dropout(attn_output)\\n        outputs = list(attention_layer_outputs[1:])\\n\\n        if self.use_parallel_residual:\\n            # pseudocode:\\n            # x = x + attn(ln1(x)) + mlp(ln2(x))\\n            mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))\\n            mlp_output = self.post_mlp_dropout(mlp_output)\\n            hidden_states = mlp_output + attn_output + hidden_states\\n            # Override the present state to be the pre-residual stream addition\\n            outputs[0] = mlp_output + attn_output\\n        else:\\n            raise NotImplementedError\\n            # pseudocode:\\n            # x = x + attn(ln1(x))\\n            # x = x + mlp(ln2(x))\\n            attn_output = attn_output + hidden_states\\n            mlp_output = self.mlp(self.post_attention_layernorm(attn_output))\\n            mlp_output = self.post_mlp_dropout(mlp_output)\\n            hidden_states = mlp_output + attn_output\\n\\n        outputs = tuple(outputs)\\n        if use_cache:\\n            outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)\\n        else:\\n            outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)\\n\\n        return outputs\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: make this a bit nicer with hooks\n",
    "# But we are changing to GPTNeoxLayer to\n",
    "\"\"\"\n",
    "   class GPTNeoXLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_parallel_residual = config.use_parallel_residual\n",
    "        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.post_attention_dropout = nn.Dropout(config.hidden_dropout)\n",
    "        self.post_mlp_dropout = nn.Dropout(config.hidden_dropout)\n",
    "        self.attention = GPTNeoXAttention(config)\n",
    "        self.mlp = GPTNeoXMLP(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[torch.FloatTensor],\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ):\n",
    "        attention_layer_outputs = self.attention(\n",
    "            self.input_layernorm(hidden_states),\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            layer_past=layer_past,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\n",
    "        attn_output = self.post_attention_dropout(attn_output)\n",
    "        outputs = list(attention_layer_outputs[1:])\n",
    "\n",
    "        if self.use_parallel_residual:\n",
    "            # pseudocode:\n",
    "            # x = x + attn(ln1(x)) + mlp(ln2(x))\n",
    "            mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))\n",
    "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
    "            hidden_states = mlp_output + attn_output + hidden_states\n",
    "            # Override the present state to be the pre-residual stream addition\n",
    "            outputs[0] = mlp_output + attn_output\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            # pseudocode:\n",
    "            # x = x + attn(ln1(x))\n",
    "            # x = x + mlp(ln2(x))\n",
    "            attn_output = attn_output + hidden_states\n",
    "            mlp_output = self.mlp(self.post_attention_layernorm(attn_output))\n",
    "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
    "            hidden_states = mlp_output + attn_output\n",
    "\n",
    "        outputs = tuple(outputs)\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \n",
      "\u001b[0mdecomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mposition_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpast_key_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCausalLMOutputWithPast\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCall signature:\u001b[0m  \u001b[0mdecomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m            partial\n",
      "\u001b[0;31mString form:\u001b[0m    \n",
      "functools.partial(<function add_hook_to_module.<locals>.new_forward at 0x72008815b760>, GPTNeoXFo <...> ntwise_affine=True)\n",
      "           )\n",
      "           (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      "           ))\n",
      "\u001b[0;31mFile:\u001b[0m            ~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\n",
      "\u001b[0;31mDocstring:\u001b[0m      \n",
      "The [`GPTNeoXForCausalLM`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "the latter silently ignores them.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "        Indices of input sequence tokens in the vocabulary.\n",
      "\n",
      "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "        [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "        [What are input IDs?](../glossary#input-ids)\n",
      "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 for tokens that are **not masked**,\n",
      "        - 0 for tokens that are **masked**.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      "        config.n_positions - 1]`.\n",
      "\n",
      "        [What are position IDs?](../glossary#position-ids)\n",
      "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "        is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n",
      "        model's internal embedding lookup matrix.\n",
      "    output_attentions (`bool`, *optional*):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "        tensors for more detail.\n",
      "    output_hidden_states (`bool`, *optional*):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "        more detail.\n",
      "    return_dict (`bool`, *optional*):\n",
      "        Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "        `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional tensors are\n",
      "        only required when the model is used as a decoder in a Sequence to Sequence model.\n",
      "\n",
      "        Contains pre-computed hidden-states (key and values in the self-attention blocks that can be used (see\n",
      "        `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
      "        `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n",
      "        ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`.\n",
      "    use_cache (`bool`, *optional*):\n",
      "        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "        `past_key_values`).\n",
      "\n",
      "\n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.CausalLMOutputWithPast`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.CausalLMOutputWithPast`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`GPTNeoXConfig`]) and inputs.\n",
      "\n",
      "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Language modeling loss (for next-token prediction).\n",
      "        - **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) -- Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
      "        - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "          `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
      "\n",
      "          Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
      "          `past_key_values` input) to speed up sequential decoding.\n",
      "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "          Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "          heads.\n",
      "  \n",
      "\n",
      "    Example:\n",
      "\n",
      "    ```python\n",
      "    >>> from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig\n",
      "    >>> import torch\n",
      "\n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "    >>> config = GPTNeoXConfig.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "    >>> config.is_decoder = True\n",
      "    >>> model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", config=config)\n",
      "\n",
      "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "    >>> outputs = model(**inputs)\n",
      "\n",
      "    >>> prediction_logits = outputs.logits\n",
      "    ```\n",
      "\u001b[0;31mClass docstring:\u001b[0m\n",
      "partial(func, *args, **keywords) - new function with partial application\n",
      "of the given arguments and keywords."
     ]
    }
   ],
   "source": [
    "decomp.model[0].forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from cache\n",
      "Got embeddings\n",
      "Using saved correlation for layer 0\n",
      "Using saved correlation for layer 1\n",
      "Using saved correlation for layer 2\n",
      "Using saved correlation for layer 3\n",
      "Using saved correlation for layer 4\n",
      "Using saved correlation for layer 5\n"
     ]
    }
   ],
   "source": [
    "decomp.load(reload=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets just look at the highest cliques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.55996289, ..., 0.55218228, 0.55726815,\n",
       "        0.54649686],\n",
       "       [0.        , 1.        , 0.420132  , ..., 0.54260334, 0.55746129,\n",
       "        0.43269434],\n",
       "       [0.55996289, 0.420132  , 1.        , ..., 0.5145273 , 0.58364553,\n",
       "        0.39925619],\n",
       "       ...,\n",
       "       [0.55218228, 0.54260334, 0.5145273 , ..., 1.        , 0.57276563,\n",
       "        0.52797111],\n",
       "       [0.55726815, 0.55746129, 0.58364553, ..., 0.57276563, 1.        ,\n",
       "        0.        ],\n",
       "       [0.54649686, 0.43269434, 0.39925619, ..., 0.52797111, 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomp.internal_correlations[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Up a \"Concept\" Lattice Using Graph Restrictions\n",
    "\n",
    "> TODO: this is for latter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.graph as graph\n",
    "import networkx as nx\n",
    "\n",
    "LAYER = 3\n",
    "\n",
    "G = graph.di_graph_from_correlations(decomp.internal_correlations[LAYER])\n",
    "MAX_CLIQUE_SIZE = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GG = graph.sparsify_weighted_digraph(G, degree_upperbound=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight': 0.8521209974147577}\n",
      "{'weight': 0.8162661745812154}\n",
      "{'weight': 0.7875908366274808}\n",
      "{'weight': 0.9736834434621171}\n",
      "{'weight': 0.7978421185982739}\n",
      "{'weight': 0.9630868645920452}\n",
      "{'weight': 0.7848217348702783}\n",
      "{'weight': 0.8309435349882173}\n",
      "{'weight': 0.8466209110905328}\n",
      "{'weight': 0.7892141804916221}\n",
      "{'weight': 0.8238873704295807}\n",
      "{'weight': 0.8249793158188582}\n",
      "{'weight': 0.9295728856730935}\n",
      "{'weight': 0.7817297580903736}\n",
      "{'weight': 0.8035910877524983}\n",
      "{'weight': 0.8094992522304162}\n",
      "{'weight': 0.780198343920196}\n",
      "{'weight': 0.7894159773808109}\n",
      "{'weight': 0.8018332126288977}\n",
      "{'weight': 0.8466276376535058}\n",
      "{'weight': 0.7833844925817222}\n",
      "{'weight': 0.8762940225519235}\n",
      "{'weight': 0.8060373144869987}\n",
      "{'weight': 0.7847163520503685}\n",
      "{'weight': 0.8033511736731294}\n",
      "{'weight': 0.8379077698528901}\n",
      "{'weight': 0.8436836452590063}\n",
      "{'weight': 0.7810526174177621}\n",
      "{'weight': 0.882417437044976}\n",
      "{'weight': 0.7973084779357523}\n",
      "{'weight': 0.7856042583627995}\n",
      "{'weight': 0.8001695093869187}\n",
      "{'weight': 0.7889675398492801}\n",
      "{'weight': 0.8732468895251719}\n",
      "{'weight': 0.8673140609830199}\n",
      "{'weight': 0.8512196379763808}\n",
      "{'weight': 0.7824921018939759}\n",
      "{'weight': 0.8476612861636842}\n",
      "{'weight': 0.8657310764967163}\n",
      "{'weight': 0.7923465166493645}\n",
      "{'weight': 0.8390221371187441}\n",
      "{'weight': 0.8583901541055577}\n",
      "{'weight': 0.8323807772767734}\n",
      "{'weight': 0.8056718378988011}\n",
      "{'weight': 0.8117257445744664}\n",
      "{'weight': 0.860674943328707}\n",
      "{'weight': 0.8675427641241006}\n",
      "{'weight': 0.8373158323112695}\n",
      "{'weight': 0.7825414300224443}\n",
      "{'weight': 0.8741908505290442}\n",
      "{'weight': 0.8100553147695143}\n"
     ]
    }
   ],
   "source": [
    "for j in range(1_024):\n",
    "    c = GG.get_edge_data(0, j)\n",
    "    if c is not None:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 962, 932, 292, 564]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7cfd8fd7beb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GG.get_edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.utils' from '/home/lev/code/research/ai/lattice_cluster_paths/src/utils.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import src.utils as utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "decomp.tune_clique((0.0, clique), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "\n",
    "# test_at = 100\n",
    "# GG = graph.sparsify_weighted_graph(G, degree_upperbound=test_at)\n",
    "\n",
    "# cliques = []\n",
    "\n",
    "# for node in range(N_DIMS * 2):\n",
    "#     if node % 50 == 0:\n",
    "#         print(\"On node\", node)\n",
    "#     c_it = nx.find_cliques(GG, nodes=[node])\n",
    "#     c_in_curr = []\n",
    "#     for c in c_it:\n",
    "#         c_in_curr.append(c)\n",
    "#         if len(c_in_curr) > 50:\n",
    "#             break\n",
    "#     cliques += c_in_curr\n",
    "#         # if len(cliques) > 10_000:\n",
    "#         #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.50411391, ..., 0.41980654, 0.47819793,\n",
       "        0.52677501],\n",
       "       [0.        , 1.        , 0.51517917, ..., 0.56102004, 0.50169823,\n",
       "        0.55460722],\n",
       "       [0.50411391, 0.51517917, 1.        , ..., 0.47680901, 0.5166343 ,\n",
       "        0.52601947],\n",
       "       ...,\n",
       "       [0.41980654, 0.56102004, 0.47680901, ..., 1.        , 0.40289529,\n",
       "        0.57754201],\n",
       "       [0.47819793, 0.50169823, 0.5166343 , ..., 0.40289529, 1.        ,\n",
       "        0.        ],\n",
       "       [0.52677501, 0.55460722, 0.52601947, ..., 0.57754201, 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomp.internal_correlations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT CLIQUES WITH LENGTH [9, 9, 10] [(23.79445227225935, [2, 771, 128, 217, 389, 593, 657, 661, 785]), (25.24844098956523, [2, 771, 128, 217, 389, 593, 657, 687, 936]), (29.81072255571707, [2, 771, 128, 217, 389, 593, 657, 687, 785, 613])]\n",
      "Got tuned clique [0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6]\n",
      "Got tuned clique [0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6, 0.0]\n",
      "Got tuned clique [0.0, 0.8, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0]\n",
      "Finished for neuron 2 2\n"
     ]
    }
   ],
   "source": [
    "import src.decorrelate as decc\n",
    "import src.kernel as kernel\n",
    "importlib.reload(decc)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(kernel)\n",
    "\n",
    "LAYER = 2\n",
    "NEURON = 2\n",
    "\n",
    "# TODO: we **need** to do something like curr-clique removal so we get more interesting cliques...\n",
    "# Also the per-neuron thing makes no sense...\n",
    "\n",
    "# TODO: change size of subset??\n",
    "decomp.scores_for_neuron(LAYER, NEURON, degree_upperbound=400, n_features_per_neuron=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
