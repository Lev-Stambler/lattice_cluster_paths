{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating decomposer with parameter data hash cache/data-9ca1a7113cd59c0eaf97b97c20dd15e5b47664cf/start.pkl\n",
      "Creating decomposer with parameter lattice hash cache/correlation-cf1e08d4f095000f1b6688a204d42b7c56a5d436/start.pkl\n",
      "Created dataset\n"
     ]
    }
   ],
   "source": [
    "import src.decorrelate as cluster_model\n",
    "MODEL_NAME = 'EleutherAI/pythia-70m'\n",
    "DATASET_NAME = 'NeelNanda/pile-10k'\n",
    "\n",
    "N_DIMS = 512\n",
    "SEED = 69_420\n",
    " \n",
    "DEBUG = False\n",
    " \n",
    "if DEBUG:\n",
    "    N_DATASIZE = 300\n",
    "    N_BLOCKS = 12\n",
    "    STRING_SIZE_CUTOFF = 200\n",
    "else:\n",
    "    # It gets killed aroun 1_800 idk why. Maybe we have a problem with token truncation somewhere\n",
    "    N_DATASIZE = 2_001 # TODO: was 3_000 for non-digraph\n",
    "# \n",
    "    # N_CLUSTERS_MIN = int(0.5 * N_DIMS)\n",
    "    # N_CLUSTERS_MAX = 10 * N_DIMS\n",
    "    # TODO: DEL ME\n",
    "    N_BLOCKS = 6\n",
    "    STRING_SIZE_CUTOFF = 1_000\n",
    "\n",
    "params = cluster_model.InterpParams(\n",
    "\tlattice_params=cluster_model.LatticeParams(\n",
    "\t\ttop_layer_idx = -1,\n",
    "        max_n_parents = 4\n",
    "\t),\n",
    "    # quantization='4bit',\n",
    "\tseed=SEED,\n",
    "    n_datasize=N_DATASIZE,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    model_name=MODEL_NAME,\n",
    "\tmodel_n_dims=N_DIMS,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    string_size_cutoff=STRING_SIZE_CUTOFF,\n",
    "    quantization='4bit'\n",
    ")\n",
    "\n",
    "decomp = cluster_model.Decomposer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n   class GPTNeoXLayer(nn.Module):\\n    def __init__(self, config):\\n        super().__init__()\\n        self.use_parallel_residual = config.use_parallel_residual\\n        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\\n        self.post_attention_dropout = nn.Dropout(config.hidden_dropout)\\n        self.post_mlp_dropout = nn.Dropout(config.hidden_dropout)\\n        self.attention = GPTNeoXAttention(config)\\n        self.mlp = GPTNeoXMLP(config)\\n\\n    def forward(\\n        self,\\n        hidden_states: Optional[torch.FloatTensor],\\n        attention_mask: Optional[torch.FloatTensor] = None,\\n        position_ids: Optional[torch.LongTensor] = None,\\n        head_mask: Optional[torch.FloatTensor] = None,\\n        use_cache: Optional[bool] = False,\\n        layer_past: Optional[Tuple[torch.Tensor]] = None,\\n        output_attentions: Optional[bool] = False,\\n    ):\\n        attention_layer_outputs = self.attention(\\n            self.input_layernorm(hidden_states),\\n            attention_mask=attention_mask,\\n            position_ids=position_ids,\\n            layer_past=layer_past,\\n            head_mask=head_mask,\\n            use_cache=use_cache,\\n            output_attentions=output_attentions,\\n        )\\n        attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\\n        attn_output = self.post_attention_dropout(attn_output)\\n        outputs = list(attention_layer_outputs[1:])\\n\\n        if self.use_parallel_residual:\\n            # pseudocode:\\n            # x = x + attn(ln1(x)) + mlp(ln2(x))\\n            mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))\\n            mlp_output = self.post_mlp_dropout(mlp_output)\\n            hidden_states = mlp_output + attn_output + hidden_states\\n            # Override the present state to be the pre-residual stream addition\\n            outputs[0] = mlp_output + attn_output\\n        else:\\n            raise NotImplementedError\\n            # pseudocode:\\n            # x = x + attn(ln1(x))\\n            # x = x + mlp(ln2(x))\\n            attn_output = attn_output + hidden_states\\n            mlp_output = self.mlp(self.post_attention_layernorm(attn_output))\\n            mlp_output = self.post_mlp_dropout(mlp_output)\\n            hidden_states = mlp_output + attn_output\\n\\n        outputs = tuple(outputs)\\n        if use_cache:\\n            outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)\\n        else:\\n            outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)\\n\\n        return outputs\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: make this a bit nicer with hooks\n",
    "# But we are changing to GPTNeoxLayer to\n",
    "\"\"\"\n",
    "   class GPTNeoXLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_parallel_residual = config.use_parallel_residual\n",
    "        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.post_attention_dropout = nn.Dropout(config.hidden_dropout)\n",
    "        self.post_mlp_dropout = nn.Dropout(config.hidden_dropout)\n",
    "        self.attention = GPTNeoXAttention(config)\n",
    "        self.mlp = GPTNeoXMLP(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: Optional[torch.FloatTensor],\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ):\n",
    "        attention_layer_outputs = self.attention(\n",
    "            self.input_layernorm(hidden_states),\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            layer_past=layer_past,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attention_layer_outputs[0]  # output_attn: attn_output, present, (attn_weights)\n",
    "        attn_output = self.post_attention_dropout(attn_output)\n",
    "        outputs = list(attention_layer_outputs[1:])\n",
    "\n",
    "        if self.use_parallel_residual:\n",
    "            # pseudocode:\n",
    "            # x = x + attn(ln1(x)) + mlp(ln2(x))\n",
    "            mlp_output = self.mlp(self.post_attention_layernorm(hidden_states))\n",
    "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
    "            hidden_states = mlp_output + attn_output + hidden_states\n",
    "            # Override the present state to be the pre-residual stream addition\n",
    "            outputs[0] = mlp_output + attn_output\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            # pseudocode:\n",
    "            # x = x + attn(ln1(x))\n",
    "            # x = x + mlp(ln2(x))\n",
    "            attn_output = attn_output + hidden_states\n",
    "            mlp_output = self.mlp(self.post_attention_layernorm(attn_output))\n",
    "            mlp_output = self.post_mlp_dropout(mlp_output)\n",
    "            hidden_states = mlp_output + attn_output\n",
    "\n",
    "        outputs = tuple(outputs)\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs  # hidden_states, present, (attn_weights)\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]  # hidden_states, (attn_weights)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \n",
      "\u001b[0mdecomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mposition_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpast_key_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCausalLMOutputWithPast\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCall signature:\u001b[0m  \u001b[0mdecomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m            partial\n",
      "\u001b[0;31mString form:\u001b[0m    \n",
      "functools.partial(<function add_hook_to_module.<locals>.new_forward at 0x7e3e786c7ac0>, GPTNeoXFo <...> ntwise_affine=True)\n",
      "           )\n",
      "           (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      "           ))\n",
      "\u001b[0;31mFile:\u001b[0m            ~/.local/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py\n",
      "\u001b[0;31mDocstring:\u001b[0m      \n",
      "The [`GPTNeoXForCausalLM`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "the latter silently ignores them.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "        Indices of input sequence tokens in the vocabulary.\n",
      "\n",
      "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "        [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "        [What are input IDs?](../glossary#input-ids)\n",
      "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 for tokens that are **not masked**,\n",
      "        - 0 for tokens that are **masked**.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      "        config.n_positions - 1]`.\n",
      "\n",
      "        [What are position IDs?](../glossary#position-ids)\n",
      "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "        is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n",
      "        model's internal embedding lookup matrix.\n",
      "    output_attentions (`bool`, *optional*):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "        tensors for more detail.\n",
      "    output_hidden_states (`bool`, *optional*):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "        more detail.\n",
      "    return_dict (`bool`, *optional*):\n",
      "        Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "        `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional tensors are\n",
      "        only required when the model is used as a decoder in a Sequence to Sequence model.\n",
      "\n",
      "        Contains pre-computed hidden-states (key and values in the self-attention blocks that can be used (see\n",
      "        `past_key_values` input) to speed up sequential decoding.\n",
      "\n",
      "        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
      "        `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n",
      "        ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`.\n",
      "    use_cache (`bool`, *optional*):\n",
      "        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "        `past_key_values`).\n",
      "\n",
      "\n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.CausalLMOutputWithPast`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.CausalLMOutputWithPast`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`GPTNeoXConfig`]) and inputs.\n",
      "\n",
      "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Language modeling loss (for next-token prediction).\n",
      "        - **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) -- Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
      "        - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "          `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
      "\n",
      "          Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
      "          `past_key_values` input) to speed up sequential decoding.\n",
      "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "          Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "          heads.\n",
      "  \n",
      "\n",
      "    Example:\n",
      "\n",
      "    ```python\n",
      "    >>> from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXConfig\n",
      "    >>> import torch\n",
      "\n",
      "    >>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "    >>> config = GPTNeoXConfig.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
      "    >>> config.is_decoder = True\n",
      "    >>> model = GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\", config=config)\n",
      "\n",
      "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "    >>> outputs = model(**inputs)\n",
      "\n",
      "    >>> prediction_logits = outputs.logits\n",
      "    ```\n",
      "\u001b[0;31mClass docstring:\u001b[0m\n",
      "partial(func, *args, **keywords) - new function with partial application\n",
      "of the given arguments and keywords."
     ]
    }
   ],
   "source": [
    "decomp.model[0].forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from cache\n",
      "Got embeddings\n",
      "Using saved correlation for layer 0\n",
      "Using saved correlation for layer 1\n",
      "Using saved correlation for layer 2\n",
      "Using saved correlation for layer 3\n",
      "Using saved correlation for layer 4\n",
      "Using saved correlation for layer 5\n"
     ]
    }
   ],
   "source": [
    "# https://sporco.readthedocs.io/en/latest/examples/dl/cmod.h\n",
    "decomp.load(reload=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to do dictionary learning on the correlation lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.model as smodel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# import SparseAutoencoder, train_autoencoder\n",
    "importlib.reload(smodel)\n",
    "\n",
    "D_SCALE = 3\n",
    "LAYER = 2\n",
    "\n",
    "if False:\n",
    "    # hidden_size = 10  # Number of hidden units\n",
    "    autoencoder = smodel.SparseAutoencoder(N_DIMS * 2, N_DIMS * 2 * D_SCALE, beta=1, lamda=0.001)\n",
    "    \n",
    "    # # Train the autoencoder\n",
    "    dataset = TensorDataset(torch.FloatTensor(decomp.internal_correlations[LAYER]))\n",
    "    smodel.train_autoencoder(autoencoder, dataset, num_epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07352224038146993, tolerance: 0.026520905002279643\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07522011663226635, tolerance: 0.02655072854893728\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.090856876140748, tolerance: 0.026579641004772534\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06291574495273267, tolerance: 0.02671430003764109\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09659548187516975, tolerance: 0.026511652652087474\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08990589847467811, tolerance: 0.02647361140756329\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07281797245987498, tolerance: 0.02654716541469509\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08364144534223783, tolerance: 0.026590921930152958\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06876123581313998, tolerance: 0.026592365786615984\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08016695286733011, tolerance: 0.026491892449768264\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08262736268721937, tolerance: 0.026501840189813993\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07661001607326079, tolerance: 0.02666781724403708\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08675696029476399, tolerance: 0.026521237123602624\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04669897242226839, tolerance: 0.026529199053690344\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07741839957500929, tolerance: 0.026494114541681702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0833719584225514, tolerance: 0.026542629218512213\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08621900503878255, tolerance: 0.026653835776070813\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09417346749319222, tolerance: 0.0265417927534808\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14448387719509892, tolerance: 0.026493147116767774\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06845749418576474, tolerance: 0.026504924609414924\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08535907810650656, tolerance: 0.026547253328783098\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.08052959411278948, tolerance: 0.02658253883529997\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0700684203010491, tolerance: 0.026489110108445277\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0919317482787676, tolerance: 0.026561933068287774\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.10810676472578962, tolerance: 0.026469529459723994\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.034302504186939586, tolerance: 0.026617611570858818\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07344774904802875, tolerance: 0.026490182270833527\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06812744419755745, tolerance: 0.026510587319000026\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05775826218109614, tolerance: 0.026547986166404934\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04952063646390581, tolerance: 0.026535090217655172\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07546683704563861, tolerance: 0.026481527259313322\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09748126396655721, tolerance: 0.02654212697006591\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07938267314776226, tolerance: 0.026534244604527916\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05407774402203325, tolerance: 0.026813290100258024\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09066719749600338, tolerance: 0.02659268530081123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04854863707279833, tolerance: 0.02648387309802192\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04291785280460658, tolerance: 0.02653252898160621\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05636165851865371, tolerance: 0.026453908591403202\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.09010154651258517, tolerance: 0.026501312296328145\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04416687052636803, tolerance: 0.026511652652087474\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04712635166863066, tolerance: 0.026592365786615984\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07744996401680737, tolerance: 0.026521237123602624\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.048249074161938355, tolerance: 0.026542629218512213\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04435674277830093, tolerance: 0.026653835776070813\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06494262859865785, tolerance: 0.0265417927534808\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04407503900678833, tolerance: 0.026493147116767774\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07459266303035861, tolerance: 0.02658253883529997\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.028981305377271838, tolerance: 0.026489110108445277\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07716941594586046, tolerance: 0.026547986166404934\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03587136966971327, tolerance: 0.026535090217655172\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03879764490465831, tolerance: 0.02659268530081123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03197369947712492, tolerance: 0.02653252898160621\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.05505546493939084, tolerance: 0.026520905002279643\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06213733244496211, tolerance: 0.02655072854893728\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0463065835575045, tolerance: 0.026579641004772534\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.043447931040134335, tolerance: 0.02671430003764109\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.038238823442753755, tolerance: 0.026491892449768264\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03362676028247691, tolerance: 0.02666781724403708\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.043739966770885985, tolerance: 0.026529199053690344\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.029459685830204307, tolerance: 0.026602521074715488\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03374275806513083, tolerance: 0.026542629218512213\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.036080206063586306, tolerance: 0.026493147116767774\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06465602733959486, tolerance: 0.026561933068287774\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06417154308434192, tolerance: 0.026490182270833527\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06467099404954957, tolerance: 0.026481527259313322\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.06474333653878261, tolerance: 0.02654212697006591\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.04572290831459713, tolerance: 0.026590921930152958\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03402608396498863, tolerance: 0.026529199053690344\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.03554118339526724, tolerance: 0.026494114541681702\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.040033403899229825, tolerance: 0.026590921930152958\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import DictionaryLearning\n",
    "import numpy as np\n",
    "\n",
    "D_SCALE = 3\n",
    "\n",
    "internal_corrs = np.array(decomp.internal_correlations[LAYER])\n",
    "for i in range(N_DIMS * 2):\n",
    "    internal_corrs[i, i] = 0.0\n",
    "\n",
    "d = DictionaryLearning(n_components=N_DIMS * 2 * D_SCALE, fit_algorithm='cd')\n",
    "dfit = d.fit(internal_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02710731, 0.03343181, 0.02971446, ..., 0.03398587, 0.04088725,\n",
       "        0.02003468],\n",
       "       [0.02770216, 0.03278088, 0.02743574, ..., 0.03570079, 0.04139068,\n",
       "        0.01948908],\n",
       "       [0.02889087, 0.03162245, 0.02747366, ..., 0.03190653, 0.04027262,\n",
       "        0.02051257],\n",
       "       ...,\n",
       "       [0.02611043, 0.03421758, 0.02939289, ..., 0.03417075, 0.04114558,\n",
       "        0.01969317],\n",
       "       [0.02858828, 0.03153736, 0.0197275 , ..., 0.0325823 , 0.04259548,\n",
       "        0.0180739 ],\n",
       "       [0.02977646, 0.03061378, 0.03379255, ..., 0.03784154, 0.03903835,\n",
       "        0.02176072]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfit.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lev/.local/lib/python3.10/site-packages/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  new_code = orthogonal_mp_gram(\n"
     ]
    }
   ],
   "source": [
    "X = dfit.transform(internal_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 1024)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfit.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "reconstr = None\n",
    "for i in X[idx].nonzero()[0]:\n",
    "    if reconstr is None:\n",
    "        reconstr = dfit.components_[i] * X[idx][i]\n",
    "    else: reconstr += dfit.components_[i] * X[idx][i]\n",
    "\n",
    "diff = np.abs(reconstr - decomp.internal_correlations[LAYER][idx]).sum()\n",
    "reconstr, decomp.internal_correlations[LAYER][idx], diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.55088077e-01,  5.29416047e-01, -3.08993208e-23, ...,\n",
       "         5.67832461e-01,  6.65286496e-01,  3.25854436e-01]),\n",
       " array([0.45508808, 0.52941605, 0.        , ..., 0.56783246, 0.6652865 ,\n",
       "        0.32585444]),\n",
       " 8.795672524374144e-13)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Up a \"Concept\" Lattice Using Graph Restrictions\n",
    "\n",
    "> TODO: this is for latter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.graph as graph\n",
    "import networkx as nx\n",
    "\n",
    "LAYER = 3\n",
    "\n",
    "G = graph.di_graph_from_correlations(decomp.internal_correlations[LAYER])\n",
    "MAX_CLIQUE_SIZE = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "GG = graph.sparsify_weighted_digraph(G, degree_upperbound=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NetworkXNotImplemented",
     "evalue": "not implemented for directed type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNetworkXNotImplemented\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connected\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGG\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/classes/backends.py:148\u001b[0m, in \u001b[0;36m_dispatch.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m NetworkXNotImplemented(\n\u001b[1;32m    146\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not implemented by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplugin_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m             )\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/utils/decorators.py:766\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[0;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 20:3\u001b[0m, in \u001b[0;36margmap_is_connected_17\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/utils/decorators.py:86\u001b[0m, in \u001b[0;36mnot_implemented_for.<locals>._not_implemented_for\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_not_implemented_for\u001b[39m(g):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (mval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m mval \u001b[38;5;241m==\u001b[39m g\u001b[38;5;241m.\u001b[39mis_multigraph()) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m     84\u001b[0m         dval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m dval \u001b[38;5;241m==\u001b[39m g\u001b[38;5;241m.\u001b[39mis_directed()\n\u001b[1;32m     85\u001b[0m     ):\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXNotImplemented(errmsg)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "\u001b[0;31mNetworkXNotImplemented\u001b[0m: not implemented for directed type"
     ]
    }
   ],
   "source": [
    "nx.is_connected(GG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7804636816254003,\n",
       " 0.7813018624377651,\n",
       " 0.7817276582904464,\n",
       " 0.7819791125341559,\n",
       " 0.782213803161618,\n",
       " 0.7823479120915964,\n",
       " 0.7833436708966859,\n",
       " 0.7835247179521566,\n",
       " 0.7869411429433557,\n",
       " 0.7878128509882152,\n",
       " 0.7910716979866896,\n",
       " 0.7912192178096659,\n",
       " 0.79138014852564,\n",
       " 0.7920272241127856,\n",
       " 0.7953866528087439,\n",
       " 0.7988600740951838,\n",
       " 0.80074430456138,\n",
       " 0.8030811526662531,\n",
       " 0.8040802641945921,\n",
       " 0.8048480378187183,\n",
       " 0.8060449600187752,\n",
       " 0.8073391111930666,\n",
       " 0.8090758218362866,\n",
       " 0.8101017551506211,\n",
       " 0.8137763398320286,\n",
       " 0.8160193116859169,\n",
       " 0.8231539067607664,\n",
       " 0.8258427908068329,\n",
       " 0.8313479623824451,\n",
       " 0.8318173436373695,\n",
       " 0.8386602517895161,\n",
       " 0.8412820813705932,\n",
       " 0.8418151643672573,\n",
       " 0.8424857090171491,\n",
       " 0.8467771947764572,\n",
       " 0.847658960991065,\n",
       " 0.8497845875312222,\n",
       " 0.8533854123011416,\n",
       " 0.8545387490989557,\n",
       " 0.8598863426818433,\n",
       " 0.8603825457227633,\n",
       " 0.8661056443095905,\n",
       " 0.8667191926642416,\n",
       " 0.8674266172698775,\n",
       " 0.8727473890667695,\n",
       " 0.8749501282416643,\n",
       " 0.8766365480361423,\n",
       " 0.8823227666672254,\n",
       " 0.9292005431411664,\n",
       " 0.962989288049218,\n",
       " 0.973047457797596]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = []\n",
    "for j in range(1_024):\n",
    "    c = GG.get_edge_data(0, j)\n",
    "    if c is not None:\n",
    "        cc.append(c['weight'])\n",
    "cc.sort()\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[43mi\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "next(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7cfd8fd7beb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GG.get_edge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.utils' from '/home/lev/code/research/ai/lattice_cluster_paths/src/utils.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import src.utils as utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "decomp.tune_clique((0.0, clique), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "\n",
    "# test_at = 100\n",
    "# GG = graph.sparsify_weighted_graph(G, degree_upperbound=test_at)\n",
    "\n",
    "# cliques = []\n",
    "\n",
    "# for node in range(N_DIMS * 2):\n",
    "#     if node % 50 == 0:\n",
    "#         print(\"On node\", node)\n",
    "#     c_it = nx.find_cliques(GG, nodes=[node])\n",
    "#     c_in_curr = []\n",
    "#     for c in c_it:\n",
    "#         c_in_curr.append(c)\n",
    "#         if len(c_in_curr) > 50:\n",
    "#             break\n",
    "#     cliques += c_in_curr\n",
    "#         # if len(cliques) > 10_000:\n",
    "#         #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.50411391, ..., 0.41980654, 0.47819793,\n",
       "        0.52677501],\n",
       "       [0.        , 1.        , 0.51517917, ..., 0.56102004, 0.50169823,\n",
       "        0.55460722],\n",
       "       [0.50411391, 0.51517917, 1.        , ..., 0.47680901, 0.5166343 ,\n",
       "        0.52601947],\n",
       "       ...,\n",
       "       [0.41980654, 0.56102004, 0.47680901, ..., 1.        , 0.40289529,\n",
       "        0.57754201],\n",
       "       [0.47819793, 0.50169823, 0.5166343 , ..., 0.40289529, 1.        ,\n",
       "        0.        ],\n",
       "       [0.52677501, 0.55460722, 0.52601947, ..., 0.57754201, 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomp.internal_correlations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT CLIQUES WITH LENGTH [9, 9, 10] [(23.79445227225935, [2, 771, 128, 217, 389, 593, 657, 661, 785]), (25.24844098956523, [2, 771, 128, 217, 389, 593, 657, 687, 936]), (29.81072255571707, [2, 771, 128, 217, 389, 593, 657, 687, 785, 613])]\n",
      "Got tuned clique [0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6]\n",
      "Got tuned clique [0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6, 0.0]\n",
      "Got tuned clique [0.0, 0.8, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0]\n",
      "Finished for neuron 2 2\n"
     ]
    }
   ],
   "source": [
    "import src.decorrelate as decc\n",
    "import src.kernel as kernel\n",
    "importlib.reload(decc)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(kernel)\n",
    "\n",
    "LAYER = 2\n",
    "NEURON = 2\n",
    "\n",
    "# TODO: we **need** to do something like curr-clique removal so we get more interesting cliques...\n",
    "# Also the per-neuron thing makes no sense...\n",
    "\n",
    "# TODO: change size of subset??\n",
    "decomp.scores_for_neuron(LAYER, NEURON, degree_upperbound=400, n_features_per_neuron=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
